Ollama is a local LLM runtime that allows you to run large language models on your machine.
It allows users to download and run pretrained models such as Gemma,
LLaMA, and Phi directly on their local machine.
Ollama provides both a command-line interface and a REST API,
making it easy to integrate LLMs into applications.
It is commonly used for offline, private, and low-latency AI systems.

